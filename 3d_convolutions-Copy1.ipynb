{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#CONSTRUCT CLASSES\n",
    "\n",
    "class Data:      \n",
    "    label = 0\n",
    "    joints=[]\n",
    "    filename =''\n",
    "    random_array = []\n",
    "    frame = 0\n",
    "    skelno = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_skeleton(filename,count):\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "  \n",
    "    frame_number = int(content[0])\n",
    "    print(\"Frame Number: \",frame_number)\n",
    "    #print(len(content))\n",
    "    #print(content[2])\n",
    "    \n",
    "    \n",
    "    new_skel = Data()\n",
    "    \n",
    "    xyz_skeletons = []\n",
    "    \n",
    "    new_skel.frame = frame_number\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    joint_number = 25\n",
    "    skeleton2 = 2\n",
    "    skeleton3 = 3\n",
    "    \n",
    "    \n",
    "    if(int(content[1])==1):\n",
    "        a=4\n",
    "        b=0\n",
    "        new_skel.skelno = 1\n",
    "        new_skel.joints =  np.zeros(((frame_number, 25, 2, 3)))\n",
    "        new_skel.label = int(filename[18:20])\n",
    "        for i in range(frame_number):\n",
    "            for j in range(joint_number):\n",
    "                splitted_data = content[a+j].split()\n",
    "                new_skel.joints[i,j,0,0] = splitted_data[0]\n",
    "                new_skel.joints[i,j,0,1] = splitted_data[1]\n",
    "                new_skel.joints[i,j,0,2] = splitted_data[2]\n",
    "                new_skel.filename = filename\n",
    "            a=a+28\n",
    "            b=b+3\n",
    "        xyz_skeletons.append(new_skel)\n",
    "        \n",
    "        \n",
    "    if(int(content[1])==2):       \n",
    "        a=4\n",
    "        b=0\n",
    "        new_skel.skelno = 2\n",
    "        new_skel.joints =  np.zeros(((frame_number, 25, 2, 3)))\n",
    "        new_skel.label = int(filename[18:20])\n",
    "        for i in range(frame_number):\n",
    "            for s in range(skeleton2):\n",
    "                for j in range(joint_number):\n",
    "                    splitted_data = content[a+j].split()\n",
    "                    new_skel.joints[i,j,s,0] = splitted_data[0]\n",
    "                    new_skel.joints[i,j,s,1] = splitted_data[1]\n",
    "                    new_skel.joints[i,j,s,2] = splitted_data[2]\n",
    "                    new_skel.filename = filename\n",
    "                if (s==0):\n",
    "                    a=a+27\n",
    "            b=b+3 \n",
    "            a=a+28           \n",
    "        xyz_skeletons.append(new_skel)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    if(int(content[1])==3):\n",
    "                \n",
    "        a=4\n",
    "        b=0\n",
    "        new_skel.joints =  np.zeros((75, 3*frame_number))\n",
    "        new_skel.label = int(filename[18:20])\n",
    "        for i in range(frame_number):\n",
    "            for s in range(skeleton3):\n",
    "                for j in range(joint_number):\n",
    "                    splitted_data = content[a+j].split()\n",
    "                    new_skel.joints[j + s*25][i][0] = splitted_data[0]\n",
    "                    new_skel.joints[j + s*25][i][1] = splitted_data[1] \n",
    "                    new_skel.joints[j + s*25][i][2] = splitted_data[2] \n",
    "                    new_skel.filename = filename\n",
    "                if (s<=1):\n",
    "                    a=a+27\n",
    "            a=a+28\n",
    "            b=b+3\n",
    "        xyz_skeletons.append(new_skel)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "    return new_skel       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import csv\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "os.chdir(\"/home/murat/Desktop/skeleton_s1/s001\")\n",
    "count=0\n",
    "\n",
    "\n",
    "skeleton_data = []\n",
    "\n",
    "for filename in glob.glob(\"*.skeleton\"): \n",
    "    \n",
    "    count = count + 1\n",
    "    print(\"++++++++++++++++++++++++++++\")\n",
    "    print(count,filename)\n",
    "    print(\"++++++++++++++++++++++++++++\")\n",
    "    skeleton_data.append(read_skeleton(filename,count))\n",
    "\n",
    "os.chdir(\"/home/murat/Desktop/ebbt_har/compact_data\")    \n",
    "pickle.dump(skeleton_data, open('skeleton_data', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#reconstruct data with reference skeleton number 2  \n",
    "\n",
    "joint_number = 25\n",
    "skeleton_number = 2 \n",
    "\n",
    "for i in range(len(skeleton_data)):\n",
    "    for j in range(skeleton_data[i].joints.shape[0]):\n",
    "        for k in range(joint_number):\n",
    "            if (k!=1):\n",
    "                for s in range(skeleton_number):                                                          \n",
    "                    skeleton_data[i].joints[j,1,s,2]\n",
    "                    skeleton_data[i].joints[j,k,s,0] = skeleton_data[i].joints[j,k,s,0] - skeleton_data[i].joints[j,1,s,0]\n",
    "                    skeleton_data[i].joints[j,k,s,1] = skeleton_data[i].joints[j,k,s,1] - skeleton_data[i].joints[j,1,s,1]\n",
    "                    skeleton_data[i].joints[j,k,s,2] = skeleton_data[i].joints[j,k,s,2] - skeleton_data[i].joints[j,1,s,2]                 \n",
    "        skeleton_data[i].joints[j,1,1,0] = 0\n",
    "        skeleton_data[i].joints[j,1,1,1] = 0\n",
    "        skeleton_data[i].joints[j,1,1,2] = 0\n",
    "        skeleton_data[i].joints[j,1,0,0] = 0\n",
    "        skeleton_data[i].joints[j,1,0,1] = 0\n",
    "        skeleton_data[i].joints[j,1,0,2] = 0\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00071687,  0.39823454, -0.08065886])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(skeleton_data[0].joints[:,3,0,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/murat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/murat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if sys.path[0] == '':\n",
      "/home/murat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/murat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/murat/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "##########  NORMALIZE DATA \n",
    "\n",
    "joint_number = 25\n",
    "\n",
    "for i in range(len(skeleton_data)):     \n",
    "    for j in range(joint_number):\n",
    "        mean_xyz = np.mean(skeleton_data[i].joints[:,j,0,:], axis=0)\n",
    "        std_xyz = np.std(skeleton_data[i].joints[:,j,0,:], axis=0)\n",
    "        for k in range(skeleton_data[i].joints.shape[0]):\n",
    "            skeleton_data[i].joints[k,j,0,0] = (skeleton_data[i].joints[k,j,0,0] - mean_xyz[0]) / std_xyz[0]\n",
    "            skeleton_data[i].joints[k,j,0,1] = (skeleton_data[i].joints[k,j,0,1] - mean_xyz[1]) / std_xyz[1]\n",
    "            skeleton_data[i].joints[k,j,0,2] = (skeleton_data[i].joints[k,j,0,2] - mean_xyz[2]) / std_xyz[2]\n",
    "            skeleton_data[i].joints[k,1,0,0] = 0\n",
    "            skeleton_data[i].joints[k,1,0,1] = 0\n",
    "            skeleton_data[i].joints[k,1,0,2] = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    if(skeleton_data[i].skelno == 2):\n",
    "        for j in range(joint_number):\n",
    "            mean_xyz = np.mean(skeleton_data[i].joints[:,j,1,:], axis=0)\n",
    "            std_xyz = np.std(skeleton_data[i].joints[:,j,1,:], axis=0)\n",
    "            for k in range(skeleton_data[i].joints.shape[0]):\n",
    "                skeleton_data[i].joints[k,j,1,0] = (skeleton_data[i].joints[k,j,1,0] - mean_xyz[0]) / std_xyz[0]\n",
    "                skeleton_data[i].joints[k,j,1,1] = (skeleton_data[i].joints[k,j,1,1] - mean_xyz[1]) / std_xyz[1]\n",
    "                skeleton_data[i].joints[k,j,1,2] = (skeleton_data[i].joints[k,j,1,2] - mean_xyz[2]) / std_xyz[2]\n",
    "                skeleton_data[i].joints[k,1,1,0] = 0\n",
    "                skeleton_data[i].joints[k,1,1,1] = 0\n",
    "                skeleton_data[i].joints[k,1,1,2] = 0\n",
    "    mean_xyz = 0\n",
    "    std_xyz = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/murat/Desktop/ebbt_har/compact_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-909d0bb510b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0msk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/murat/Desktop/ebbt_har/compact_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/murat/Desktop/ebbt_har/compact_data'"
     ]
    }
   ],
   "source": [
    "#choose random 100 frame of the video\n",
    "\n",
    "sequence_number = 100\n",
    "joint_number = 25\n",
    "skeleton_number = 2\n",
    "\n",
    "sk = []\n",
    "\n",
    "\n",
    "for i in range(len(skeleton_data)):\n",
    "    \n",
    "    seq = Data()\n",
    "    \n",
    "    seq.joints =  np.zeros(((sequence_number,25,2,3)))\n",
    "    \n",
    "    if (skeleton_data[i].frame <= sequence_number):\n",
    "        rd = np.arange(skeleton_data[i].frame)\n",
    "        for j in range(skeleton_data[i].frame):\n",
    "            for k in range(joint_number):\n",
    "                for m in range(skeleton_number):\n",
    "                    seq.joints[j + int((sequence_number - skeleton_data[i].frame)/2), k, m, 0] = skeleton_data[i].joints[rd[j], k, m,0]\n",
    "                    seq.joints[j + int((sequence_number - skeleton_data[i].frame)/2), k, m, 1] = skeleton_data[i].joints[rd[j], k, m,1]\n",
    "                    seq.joints[j + int((sequence_number - skeleton_data[i].frame)/2), k, m, 2] = skeleton_data[i].joints[rd[j], k, m,2]\n",
    "                    seq.filename = skeleton_data[i].filename \n",
    "                    seq.label = skeleton_data[i].label\n",
    "                    seq.random_array = rd\n",
    "                    seq.frame = skeleton_data[i].frame\n",
    "        sk.append(seq)\n",
    "        \n",
    "    else:\n",
    "        rd = np.random.choice(skeleton_data[i].frame, sequence_number, replace=False)\n",
    "        rd.sort()\n",
    "        for j in range(sequence_number):\n",
    "            for k in range(joint_number):\n",
    "                for m in range(skeleton_number):\n",
    "                    seq.joints[j,k,m,0] = skeleton_data[i].joints[rd[j],k,m,0]\n",
    "                    seq.joints[j,k,m,1] = skeleton_data[i].joints[rd[j],k,m,1]\n",
    "                    seq.joints[j,k,m,2] = skeleton_data[i].joints[rd[j],k,m,2]\n",
    "                    seq.filename = skeleton_data[i].filename \n",
    "                    seq.label = skeleton_data[i].label\n",
    "                    seq.random_array = rd\n",
    "                    seq.frame = skeleton_data[i].frame\n",
    "        sk.append(seq)\n",
    "    \n",
    "os.chdir(\"/home/murat/Desktop/ebbt_har/compact_data\")    \n",
    "pickle.dump(sk, open('sk', 'wb'))   \n",
    "\n",
    "# minimum frame number\n",
    "\n",
    "\n",
    "m = sk[0].frame\n",
    "for i in range(len(sk)):\n",
    "    if (sk[i].frame<=m):\n",
    "        m = sk[i].frame\n",
    "print(\"minimum frame number: \",m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train joint (100, 25, 2, 3)\n",
      "test joint (100, 25, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "train = sk[:5000]\n",
    "test =  sk[5000:]\n",
    "print('train joint', train[0].joints.shape)\n",
    "print('test joint', test[0].joints.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/murat/Desktop/ebbt_har/compact_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cb04c5915198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/murat/Desktop/ebbt_har/compact_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/murat/Desktop/ebbt_har/compact_data'"
     ]
    }
   ],
   "source": [
    "#construct one hot vectors\n",
    "\n",
    "\n",
    "train_data = np.zeros(((((len(train),sequence_number,joint_number,2,3)))))\n",
    "train_label = np.zeros((len(train),60))\n",
    "\n",
    "\n",
    "for i in range(len(train)):\n",
    "    for j in range(train[i].joints.shape[0]):\n",
    "        for k in range(train[i].joints.shape[1]):\n",
    "            for m in range(skeleton_number):\n",
    "                train_data[i,j,k,m,0] =train[i].joints[j,k,m,0]\n",
    "                train_data[i,j,k,m,1] =train[i].joints[j,k,m,1]\n",
    "                train_data[i,j,k,m,2] =train[i].joints[j,k,m,2]\n",
    "    one_hot_vector = np.zeros(60)\n",
    "    one_hot_vector[train[i].label - 1] = 1\n",
    "    train_label[i] = one_hot_vector\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "test_data = np.zeros(((((len(test),sequence_number,joint_number,2,3)))))\n",
    "test_label = np.zeros((len(test),60))\n",
    "\n",
    "for i in range(len(test)):\n",
    "    for j in range(test[i].joints.shape[0]):\n",
    "        for k in range(test[i].joints.shape[1]):\n",
    "            for m in range(skeleton_number):\n",
    "                test_data[i,j,k,m,0] =test[i].joints[j,k,m,0]\n",
    "                test_data[i,j,k,m,1] =test[i].joints[j,k,m,1]\n",
    "                test_data[i,j,k,m,2] =test[i].joints[j,k,m,2]\n",
    "    one_hot_vector = np.zeros(60)\n",
    "    one_hot_vector[test[i].label - 1] = 1\n",
    "    test_label[i] = one_hot_vector\n",
    "    \n",
    "    \n",
    "os.chdir(\"/home/murat/Desktop/ebbt_har/compact_data\")    \n",
    "\n",
    "pickle.dump(train_label, open('train_label', 'wb')) \n",
    " \n",
    "pickle.dump(test_label, open('test_label', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/murat/Desktop/ebbt_har/compact_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f5f9fe25b0da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/murat/Desktop/ebbt_har/compact_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/murat/Desktop/ebbt_har/compact_data'"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/home/murat/Desktop/ebbt_har/compact_data\")    \n",
    "\n",
    "train_label = train_label.astype('float32')\n",
    "test_label = test_label.astype('float32')\n",
    "pickle.dump(train_label, open('train_label', 'wb')) \n",
    "pickle.dump(test_label, open('test_label', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.astype('float32')\n",
    "test_data = test_data.astype('float32')\n",
    "pickle.dump(train_data, open('train_data', 'wb'))\n",
    "pickle.dump(test_data, open('test_data', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 100, 25, 2, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.get_shape of <tf.Tensor 'Placeholder_6:0' shape=(?, 100, 25, 2, 3) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Relu_8:0' shape=(?, 100, 25, 2, 64) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'MaxPool3D_4:0' shape=(?, 100, 13, 1, 64) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Relu_9:0' shape=(?, 100, 13, 1, 128) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'MaxPool3D_5:0' shape=(?, 100, 7, 1, 128) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Reshape_8:0' shape=(?, 89600) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Relu_10:0' shape=(?, 1024) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'Relu_11:0' shape=(?, 512) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'dropout_2/mul:0' shape=(?, 512) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'add_14:0' shape=(?, 60) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "# A simple CNN to predict certain characteristics of the human subject from MRI images.\n",
    "# 3d convolution is used in each layer.\n",
    "# Reference: https://www.tensorflow.org/get_started/mnist/pros, http://blog.naver.com/kjpark79/220783765651\n",
    "# Adjust needed for your dataset e.g., max pooling, convolution parameters, training_step, batch size, etc\n",
    "\n",
    "width = train_data.shape[1]\n",
    "height = train_data.shape[2]\n",
    "depth = train_data.shape[3]\n",
    "nLabel = 60\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "\n",
    "# Start TensorFlow InteractiveSession\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Placeholders (MNIST image:28x28pixels=784, label=10)\n",
    "x_image = tf.placeholder(tf.float32, shape=[None, width,height,depth,3]) # [None, 28*28]\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, nLabel])  # [None, 10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#kernel_conv1 = tf.Variable(tf.truncated_normal([3, 3, 3, 3, 32], dtype=tf.float32, stddev=0.1))\n",
    "#biases_conv1 = tf.Variable(tf.constant(0.1, shape=[32], dtype=tf.float32))\n",
    "#kernel_conv2 = tf.Variable(tf.truncated_normal([3, 3, 3, 32, 64], dtype=tf.float32, stddev=0.1))\n",
    "#biases_conv2 = tf.Variable(tf.constant(0.1, shape=[64], dtype=tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "## Weight Initialization\n",
    "# Create lots of weights and biases & Initialize with a small positive number as we will use ReLU\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "## Convolution and Pooling\n",
    "# Convolution here: stride=1, zero-padded -> output size = input size\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1, 1, 1, 1, 1], padding='SAME') # conv2d, [1, 1, 1, 1]\n",
    "\n",
    "# Pooling: max pooling over 2x2 blocks\n",
    "def max_pool_2x2(x):  # tf.nn.max_pool. ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1]\n",
    "    return tf.nn.max_pool3d(x, ksize=[1, 2, 2, 2, 1], strides=[1, 1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## First Convolutional Layer\n",
    "# Conv then Max-pooling. 1st layer will have 32 features for each 5x5 patch. (1 feature -> 32 features)\n",
    "W_conv1 = weight_variable([2, 2, 2, 3, 64])  # shape of weight tensor = [5,5,1,32]\n",
    "b_conv1 = bias_variable([64])  # bias vector for each output channel. = [32]\n",
    "\n",
    "# Reshape 'x' to a 4D tensor (2nd dim=image width, 3rd dim=image height, 4th dim=nColorChannel)\n",
    "#x_image = tf.reshape(x, [-1,width,height,depth,3]) # [-1,28,28,1]\n",
    "print(x_image.get_shape) # (?, 256, 256, 40, 1)  # -> output image: 28x28 x1\n",
    "\n",
    "# x_image * weight tensor + bias -> apply ReLU -> apply max-pool\n",
    "h_conv1 = tf.nn.relu(conv3d(x_image, W_conv1) + b_conv1)  # conv2d, ReLU(x_image * weight + bias)\n",
    "print(h_conv1.get_shape) # (?, 256, 256, 40, 32)  # -> output image: 28x28 x32\n",
    "h_pool1 = max_pool_2x2(h_conv1)  # apply max-pool \n",
    "print(h_pool1.get_shape) # (?, 128, 128, 20, 32)  # -> output image: 14x14 x32\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Second Convolutional Layer\n",
    "# Conv then Max-pooling. 2nd layer will have 64 features for each 5x5 patch. (32 features -> 64 features)\n",
    "W_conv2 = weight_variable([2, 2, 2, 64, 128]) # [5, 5, 32, 64]\n",
    "b_conv2 = bias_variable([128]) # [64]\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv3d(h_pool1, W_conv2) + b_conv2)  # conv2d, .ReLU(x_image * weight + bias)\n",
    "print(h_conv2.get_shape) # (?, 128, 128, 20, 64)  # -> output image: 14x14 x64\n",
    "h_pool2 = max_pool_2x2(h_conv2)  # apply max-pool \n",
    "print(h_pool2.get_shape) # (?, 64, 64, 10, 64)    # -> output image: 7x7 x64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Densely Connected Layer (or fully-connected layer)\n",
    "# fully-connected layer with 1024 neurons to process on the entire image\n",
    "W_fc1 = weight_variable([100*7*1*128, 1024])  # [7*7*64, 1024]\n",
    "b_fc1 = bias_variable([1024]) # [1024]]\n",
    "\n",
    "W_fc2 = weight_variable([1024, 512])  # [7*7*64, 1024]\n",
    "b_fc2 = bias_variable([512]) # [1024]]\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 100*7*1*128])  # -> output image: [-1, 7*7*64] = 3136\n",
    "print(h_pool2_flat.get_shape)  # (?, 2621440)\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  # ReLU(h_pool2_flat x weight + bias)\n",
    "print(h_fc1.get_shape) # (?, 1024)  # -> output: 1024\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)  # ReLU(h_pool2_flat x weight + bias)\n",
    "print(h_fc2.get_shape) # (?, 1024)  # -> output: 1024\n",
    "\n",
    "\n",
    "## Dropout (to reduce overfitting; useful when training very large neural network)\n",
    "# We will turn on dropout during training & turn off during testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "print(h_fc2_drop.get_shape)  # -> output: 1024\n",
    "\n",
    "\n",
    "## Readout Layer\n",
    "W_fc3 = weight_variable([512, nLabel]) # [1024, 10]\n",
    "b_fc3 = bias_variable([nLabel]) # [10]\n",
    "\n",
    "y_conv = tf.matmul(h_fc2_drop, W_fc3) + b_fc3\n",
    "print(y_conv.get_shape)  # -> output: 10\n",
    "\n",
    "\n",
    "\n",
    "## Train and Evaluate the Model\n",
    "# set up for optimization (optimizer:ADAM)\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)  # 1e-4\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "print (\"Variables was initialized\")\n",
    "\n",
    "num_steps = 100   #115 #258\n",
    "epochs = 7 \n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_dataset_rand, train_labels_rand = randomize(np.array(train_data), train_label)\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size)% (train_labels_rand.shape[0] - batch_size) \n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict = {x_image : train_dataset_rand[offset:(offset + batch_size)], y_ : train_labels_rand[offset:(offset + batch_size)], keep_prob: 1.0})\n",
    "            print(\"epoch %d, step %d, train set batch accuracy %.1f\"%(e+1, step, train_accuracy * 100))\n",
    "            \n",
    "        if step % 15 ==0:\n",
    "            valid_accuracy = accuracy.eval(feed_dict={x_image : test_data, y_ : test_label, keep_prob: 1.0})\n",
    "            print (\"Validation accuracy: %.1f%%\" % (valid_accuracy * 100))\n",
    "            \n",
    "            \n",
    "        train_step.run(feed_dict = {x_image : train_dataset_rand[offset:(offset + batch_size)], y_ : train_labels_rand[offset:(offset + batch_size)], keep_prob: 0.5})\n",
    "            \n",
    "print(\"TRAINING WAS COMPLETED!\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
